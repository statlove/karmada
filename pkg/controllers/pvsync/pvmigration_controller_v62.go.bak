package pvsync

import (
	"context"
	"fmt"
	"time"
	"encoding/json"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"k8s.io/klog/v2"
	"sigs.k8s.io/yaml"

	workv1alpha1 "github.com/karmada-io/karmada/pkg/apis/work/v1alpha1"
	workv1alpha2 "github.com/karmada-io/karmada/pkg/apis/work/v1alpha2"
	"github.com/karmada-io/karmada/pkg/util"
	"github.com/karmada-io/karmada/pkg/util/helper"
	"github.com/karmada-io/karmada/pkg/util/names"
	"github.com/karmada-io/karmada/pkg/controllers/ctrlutil"
)

const PVMigrationControllerName = "pv-migration-controller"

type PVMigrationController struct {
	client.Client
	EventRecorder record.EventRecorder
	RESTMapper    meta.RESTMapper
}

func (c *PVMigrationController) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	klog.Infof("[PVMigrationController] Reconciling ResourceBinding")

	rb := &workv1alpha2.ResourceBinding{}
	if err := c.Client.Get(ctx, req.NamespacedName, rb); err != nil {
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}

	// 1. Check target is StatefulSet
	if rb.Spec.Resource.Kind != "StatefulSet" || rb.Spec.Resource.APIVersion != "apps/v1" {
		return ctrl.Result{}, nil
	}

	// 2. Check if any cluster reports "Unhealthy"
	unhealthy := false
	for _, s := range rb.Status.AggregatedStatus {
		if s.Health == "Unhealthy" {
			unhealthy = true
			break
		}
	}
	if !unhealthy {
		return ctrl.Result{}, nil
	}

	// 3. Check if dispatching is suspended
	if rb.Spec.Suspension != nil && rb.Spec.Suspension.Dispatching != nil && *rb.Spec.Suspension.Dispatching {
		klog.Infof("[Matched RB] %s/%s: Unhealthy StatefulSet, dispatching suspended", rb.Namespace, rb.Name)
	}
	
	// 4. Build source StatefulSet key
	stsKey := rb.Spec.Resource.Namespace + "." + rb.Spec.Resource.Name

	// 5. Get PV metadata Works created by PVSyncController
	workList := &workv1alpha1.WorkList{}
	if err := c.Client.List(ctx, workList, client.MatchingLabels{
		"pvsync.karmada.io/type":       "metadata",
		"pvsync.karmada.io/source-sts": stsKey,
	}); err != nil {
		klog.Errorf("Failed to list PV metadata Works for %s: %v", stsKey, err)
		return ctrl.Result{}, err
	}

	// 6. Extract clusters from RB spec
	currentClusterSet := map[string]bool{}
	for _, cluster := range rb.Spec.Clusters {
		currentClusterSet[cluster.Name] = true
	}

	// 7. Extract clusters from metadata Work namespace
	metaWorkClusterSet := map[string]bool{}
	for _, w := range workList.Items {
		clusterName, err := names.GetClusterName(w.Namespace)
		if err != nil {
			continue
		}
		metaWorkClusterSet[clusterName] = true
	}

	// 8. Compare clusters
	if !equalStringSet(currentClusterSet, metaWorkClusterSet) {
		newClusters := difference(currentClusterSet, metaWorkClusterSet)
		klog.Infof("[Scheduling Changed] %s/%s: Ïä§ÏºÄÏ§ÑÎßÅ Í≤∞Í≥ºÍ∞Ä Î∞îÎÄåÏóàÍ≥† Ïû¨Ïä§ÏºÄÏ§ÑÎßÅÎêú ÌÅ¥Îü¨Ïä§ÌÑ∞Îäî %v", rb.Namespace, rb.Name, newClusters)
		// 9. View PV-metadata work configmap
		for _, cluster := range newClusters {
			for _, w := range workList.Items {
				//ms: 4/17 modify.Check only the Work matching current cluster
				labelValue := w.Labels["pvsync.karmada.io/source-sts"]
    				expected := fmt.Sprintf("%s.%s", rb.Spec.Resource.Namespace, rb.Spec.Resource.Name)
				if labelValue != expected {
        				klog.Infof("‚õî Skipping unrelated PV metadata Work: %s (label: %s)", w.Name, labelValue)
        				continue
			    	}
				for _, manifest := range w.Spec.Workload.Manifests {
					var u unstructured.Unstructured
					if err := yaml.Unmarshal(manifest.Raw, &u); err != nil {
						klog.Warningf("Failed to parse manifest as Unstructured: %v", err)
						continue
					}
					if u.GetKind() != "ConfigMap" {
						continue
					}
	
					var cm corev1.ConfigMap
					if err := yaml.Unmarshal(manifest.Raw, &cm); err != nil {
						klog.Warningf("Failed to decode embedded ConfigMap: %v", err)
						continue
					}
					// 10. Create PV work
					for pvName, pvSpecYaml := range cm.Data {
						err := c.createPVWork(ctx, cluster, rb, pvName, pvSpecYaml)
						if err != nil {
							klog.Errorf("Failed to create PV Work for %s in cluster %s: %v", pvName, cluster, err)
						}
				
					}
				}
			}
		}
	}
	
	c.deleteSuspension(ctx, rb, workList, currentClusterSet)

	removedClusters := difference(metaWorkClusterSet, currentClusterSet)
	for _, cluster := range removedClusters {
		cleanupClusterResources(ctx, c, cluster, rb, workList)
	}

	return ctrl.Result{}, nil
}
//ms: function for 8
func equalStringSet(a, b map[string]bool) bool {
	if len(a) != len(b) {
		return false
	}
	for k := range a {
		if !b[k] {
			return false
		}
	}
	return true
}
//ms: function for 8
func difference(a, b map[string]bool) []string {
	var diff []string
	for k := range a {
		if !b[k] {
			diff = append(diff, k)
		}
	}
	return diff
}
//ms: function for 10
func (c *PVMigrationController) createPVWork(ctx context.Context, clusterName string, rb *workv1alpha2.ResourceBinding, pvName string, pvSpecYaml string) error {
	var fullpvSpec corev1.PersistentVolumeSpec
	if err := yaml.Unmarshal([]byte(pvSpecYaml), &fullpvSpec); err != nil {
		klog.Errorf("Failed to unmarshal PV spec YAML: %v", err)
		return err
	}
	var claimRef *corev1.ObjectReference
	if fullpvSpec.ClaimRef != nil {
		claimRef = &corev1.ObjectReference{
			APIVersion: fullpvSpec.ClaimRef.APIVersion,
			Kind:       fullpvSpec.ClaimRef.Kind,
			Name:       fullpvSpec.ClaimRef.Name,
			Namespace:  fullpvSpec.ClaimRef.Namespace,
		}
	}
	//ms: just copy selected field that we want
	pvSpec := corev1.PersistentVolumeSpec{
		AccessModes:                   fullpvSpec.AccessModes,
		Capacity:                      fullpvSpec.Capacity,
		ClaimRef:                      claimRef,
		PersistentVolumeReclaimPolicy: fullpvSpec.PersistentVolumeReclaimPolicy,
		StorageClassName:              fullpvSpec.StorageClassName,
		VolumeMode:                    fullpvSpec.VolumeMode,
		PersistentVolumeSource:        corev1.PersistentVolumeSource{
        		NFS: fullpvSpec.PersistentVolumeSource.NFS, 
    		},	
	}
	//ms: if pv work existed, skip
	workName := fmt.Sprintf("pv-work-%s-%s-%s", rb.Name, clusterName,pvName) //ms: modify work name
	workNamespace := names.GenerateExecutionSpaceName(clusterName)
	existing := &workv1alpha1.Work{}
	if err := c.Client.Get(ctx, client.ObjectKey{
		Name:      workName,
		Namespace: workNamespace,
	}, existing); err == nil {
		klog.Infof("üîÅ PV Work already exists for cluster %s. Skipping creation.", clusterName)
		return nil
	}
	//ms: create pv name
	generatedName := fmt.Sprintf("pv-%s-%s-%s", rb.Name, clusterName, pvName)
	var existingPVs corev1.PersistentVolumeList
	if err := c.Client.List(ctx, &existingPVs); err == nil {
		for _, existingPV := range existingPVs.Items {
			if existingPV.Spec.ClaimRef != nil &&
				claimRef != nil &&
				existingPV.Spec.ClaimRef.Name == claimRef.Name &&
				existingPV.Spec.ClaimRef.Namespace == claimRef.Namespace {
					klog.Infof("üîÅ PV for PVC %s/%s already exists. Skipping creation.", claimRef.Namespace, claimRef.Name)
					return nil
				}
		}
	}
	newPV := &corev1.PersistentVolume{
		TypeMeta: metav1.TypeMeta{
			Kind:       "PersistentVolume",
			APIVersion: "v1",
		},
		ObjectMeta: metav1.ObjectMeta{
			Name: generatedName,
		},
		Spec: pvSpec,
	}
	
	unstructuredPV, err := helper.ToUnstructured(newPV)
	if err != nil {
		klog.Errorf("Failed to convert PV to unstructured: %v", err)
		return err
	}

	workMeta := metav1.ObjectMeta{
		Name:      fmt.Sprintf("pv-work-%s-%s-%s", rb.Name, clusterName, pvName),
		Namespace: names.GenerateExecutionSpaceName(clusterName),
		Labels: map[string]string{
			"pvsync.karmada.io/type":       "pv-deployment",
			"pvsync.karmada.io/source-sts": rb.Spec.Resource.Namespace + "." + rb.Spec.Resource.Name,
			"pvsync.karmada.io/source-rb":  rb.Name,
			"pvsync.karmada.io/source-pv":  pvName,
		},
	}

	if err := ctrlutil.CreateOrUpdateWork(ctx, c.Client, workMeta, unstructuredPV); err != nil {
		klog.Errorf("Failed to create PV Work for cluster %s: %v", clusterName, err)
		return err
	}

	klog.Infof("Created PV Work for cluster %s using PV %s", clusterName, newPV.Name)
	return nil
}
func (c *PVMigrationController) deleteSuspension(ctx context.Context,rb *workv1alpha2.ResourceBinding,workList *workv1alpha1.WorkList,currentClusterSet map[string]bool,) error {
	klog.Infof("cleanup start")
	pvWorkList := &workv1alpha1.WorkList{}
	if err := c.Client.List(ctx, pvWorkList, client.MatchingLabels{
		"pvsync.karmada.io/type":      "pv-deployment",
		"pvsync.karmada.io/source-rb": rb.Name,
	}); err != nil {
		return fmt.Errorf("failed to list PV Works: %w", err)
	}
	const (
		maxRetries    = 10
		retryInterval = 2 * time.Second
	)
	availableCount := 0
	expectedCount := len(pvWorkList.Items)

	for _, pvWork := range pvWorkList.Items {
		klog.Infof("üîç Checking PV Work: %s/%s", pvWork.Namespace, pvWork.Name)

		isAvailable := false
		for i := 0; i < maxRetries; i++ {
			// ÏµúÏã† ÏÉÅÌÉú Í∞ÄÏ†∏Ïò§Í∏∞
			latest := &workv1alpha1.Work{}
			if err := c.Client.Get(ctx, client.ObjectKeyFromObject(&pvWork), latest); err != nil {
				klog.Warningf("Failed to re-fetch PV Work: %v", err)
				break
			}

			for _, m := range latest.Status.ManifestStatuses {
				if m.Identifier.Kind != "PersistentVolume" || m.Health != "Healthy" {
					continue
				}
				var phaseStruct struct {
					Phase string `json:"phase"`
				}
				if err := json.Unmarshal(m.Status.Raw, &phaseStruct); err != nil {
					klog.Warningf("Failed to parse status.phase: %v", err)
					continue
				}
				klog.Infof("Retry %d: phase = %s", i+1, phaseStruct.Phase)

				if phaseStruct.Phase == "Available"|| phaseStruct.Phase == "Bound" {
					isAvailable = true
					break
				}
			}

			if isAvailable {
				break
			}
			time.Sleep(retryInterval)
		}
		if isAvailable {
			availableCount++
		} else {
			klog.Warningf("‚è≥ PV Work %s/%s not ready after retries, skipping cleanup", pvWork.Namespace, pvWork.Name)
		}

		// ‚úÖ11.1. suspension delete first
		if availableCount == expectedCount {
			if rb.Spec.Suspension != nil && rb.Spec.Suspension.Dispatching != nil && *rb.Spec.Suspension.Dispatching {
				freshRB := &workv1alpha2.ResourceBinding{}
				if err := c.Client.Get(ctx, client.ObjectKeyFromObject(rb), freshRB); err != nil {
					klog.Errorf("‚ùå Failed to get fresh ResourceBinding: %v", err)
					return err
				}
				if freshRB.Spec.Suspension == nil {
					freshRB.Spec.Suspension = &workv1alpha2.Suspension{}
				}
				falseVal := false
				freshRB.Spec.Suspension.Dispatching = &falseVal
	
				if err := c.Client.Update(ctx, freshRB); err != nil {
					klog.Warningf("‚ùå Still failed to update suspension.dispatching: %v", err)
					return err
				} else {
					klog.Infof("‚úÖ Successfully updated suspension.dispatching to false")
				}
			}
		}
	}
	return nil
}
func cleanupClusterResources(ctx context.Context, ctrl *PVMigrationController, cluster string, rb *workv1alpha2.ResourceBinding, workList *workv1alpha1.WorkList) {
	// Ï°∞Í±¥: Î™®Îì† PV WorkÍ∞Ä Available/Bound ÏÉÅÌÉú && Î™®Îì† AggregatedStatusÍ∞Ä Healthy
	pvWorkList := &workv1alpha1.WorkList{}
	_ = ctrl.Client.List(ctx, pvWorkList, client.MatchingLabels{
		"pvsync.karmada.io/type":      "pv-deployment",
		"pvsync.karmada.io/source-rb": rb.Name,
	})

	availableCount := 0
	expectedCount := len(pvWorkList.Items)
	for _, pvWork := range pvWorkList.Items {
		for _, m := range pvWork.Status.ManifestStatuses {
			if m.Identifier.Kind != "PersistentVolume" || m.Health != "Healthy" {
				continue
			}
			var phaseStruct struct {
				Phase string `json:"phase"`
			}
			_ = json.Unmarshal(m.Status.Raw, &phaseStruct)
			if phaseStruct.Phase == "Available" || phaseStruct.Phase == "Bound" {
				availableCount++
				break
			}
		}
	}
	
	rbHealthy := false
	for _, s := range rb.Status.AggregatedStatus {
		if s.ClusterName == cluster && s.Health == "Healthy" {
			rbHealthy = true
			break
		}
	}

	if availableCount != expectedCount || !rbHealthy {
		klog.Infof("Skipping cleanup for cluster %s: PVs ready: %d/%d, RB[%s] Healthy: %v", cluster, availableCount, expectedCount, cluster, rbHealthy)
		return
	}

	klog.Infof("Cleaning up cluster %s: PVs ready and RB is Healthy", cluster)
	dynamicClient, err := util.NewClusterDynamicClientSet(cluster, ctrl.Client)
	if err != nil {
		klog.Warningf("Failed to create dynamic client for cluster %s: %v", cluster, err)
		return
	}
	for _, w := range workList.Items {
		workCluster, err := names.GetClusterName(w.Namespace)
		if err != nil || workCluster != cluster {
			continue
		}
		workType := w.Labels["pvsync.karmada.io/type"]
		if workType != "metadata" && workType != "pv-deployment" {
			continue
		}
		for _, manifest := range w.Spec.Workload.Manifests {
			var u unstructured.Unstructured
			if err := yaml.Unmarshal(manifest.Raw, &u); err != nil {
				continue
			}
			if u.GetKind() != "ConfigMap" {
				continue
			}
			var cm corev1.ConfigMap
			if err := yaml.Unmarshal(manifest.Raw, &cm); err != nil {
				continue
			}
			for pvName, pvSpecYaml := range cm.Data {
				var fullpvSpec corev1.PersistentVolumeSpec
				if err := yaml.Unmarshal([]byte(pvSpecYaml), &fullpvSpec); err != nil {
					continue
				}
				if fullpvSpec.ClaimRef != nil {
					claimGVR := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "persistentvolumeclaims"}
					dynamicClient.DynamicClientSet.Resource(claimGVR).Namespace(fullpvSpec.ClaimRef.Namespace).Delete(ctx, fullpvSpec.ClaimRef.Name, metav1.DeleteOptions{})
				}
				pvGVR := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "persistentvolumes"}
				dynamicClient.DynamicClientSet.Resource(pvGVR).Delete(ctx, pvName, metav1.DeleteOptions{})
			}
		}
		ctrl.Client.Delete(ctx, &w)
	}
}
func (c *PVMigrationController) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		Named(PVMigrationControllerName).
		For(&workv1alpha2.ResourceBinding{}). // no predicate, triggers on create/update
		Complete(c)
}
